# ğŸš€ Getting Started with Songo RL

## What You Have

A complete **AlphaZero-style Reinforcement Learning system** for Songo:

âœ… **Self-play engine** - AI plays against itself to generate training data
âœ… **MCTS** - Monte Carlo Tree Search for move selection
âœ… **Neural Network** - Policy + Value dual-head architecture
âœ… **Training pipeline** - Automated training loop with checkpointing
âœ… **Export tools** - Convert trained models to browser-compatible format

## ğŸ¯ Goal

Train an AI that:
- **Discovers optimal strategies** through self-play
- **Surpasses human players** by exploring millions of positions
- **Runs in the browser** after export

## ğŸ“‹ Prerequisites

- **Python 3.8+** ([Download](https://www.python.org/downloads/))
- **6-12 hours** of compute time (can run in background)
- **~2GB** disk space for checkpoints

## ğŸƒ Quick Test (15 minutes)

Test that everything works:

### Windows
```cmd
cd training
quick_start.bat
```

### Linux/Mac
```bash
cd training
bash quick_start.sh
```

This runs a **small 10-iteration training** to verify setup.

## ğŸ’ª Full Training

### Option 1: Default (Good Balance)

```bash
python train.py
```

- **Time:** ~12 hours
- **Quality:** Strong AI, beats 95%+ of players
- **Iterations:** 100
- **MCTS sims:** 100/move

### Option 2: Fast (Testing/Prototyping)

```bash
python train.py \
  --network-size small \
  --num-iterations 50 \
  --mcts-simulations 50
```

- **Time:** ~3 hours
- **Quality:** Good AI, beats random ~90%
- **Use case:** Quick experiments

### Option 3: Maximum Strength

```bash
python train.py \
  --hidden-size 512 \
  --num-iterations 200 \
  --mcts-simulations 200
```

- **Time:** ~24-48 hours
- **Quality:** Near-optimal play
- **Use case:** Production-ready beast

## ğŸ“Š Monitoring Progress

While training, you'll see:

```
ITERATION 15/100
==================
[1/3] Self-Play (50 games)
  Game 10/50 - Moves: 42 - Result: 1-0
  ...
  Total examples: 2100

[2/3] Training
  Total Loss: 1.234
  Policy Loss: 0.567
  Value Loss: 0.667

[3/3] Evaluation (vs Random)
  Results: W18 D1 L1
  Win rate: 90.0%  ğŸ‘ˆ This should increase over time
```

**Good signs:**
- Loss decreasing over time
- Win rate increasing (should reach 95%+)
- Policy loss < 1.0 after ~20 iterations

**Bad signs:**
- Win rate stuck below 60% â†’ Increase MCTS sims
- Loss increasing â†’ Lower learning rate
- Training too slow â†’ Reduce MCTS sims or use smaller network

## ğŸ’¾ Checkpoints

Saved in `checkpoints/`:
- `songo_latest.pt` - Most recent (auto-updated)
- `songo_iter10.pt`, `songo_iter20.pt`, ... - Periodic saves

**Resume training:**
```bash
python train.py --resume-from checkpoints/songo_latest.pt
```

## ğŸŒ Export for Browser

After training:

```bash
python export_model.py \
  --checkpoint checkpoints/songo_latest.pt \
  --output-dir ../public/models
```

Creates:
- `songo_model.onnx` - Best for production
- `songo_model.json` - For custom inference
- `inference_example.ts` - Integration guide

## ğŸ”§ Troubleshooting

### "ModuleNotFoundError: No module named 'torch'"
```bash
pip install torch numpy
```

### "Training is too slow"
**Reduce MCTS simulations:**
```bash
python train.py --mcts-simulations 50
```

**Use smaller network:**
```bash
python train.py --network-size small --hidden-size 128
```

### "Out of memory"
**Reduce batch size:**
```bash
python train.py --batch-size 32
```

### "Win rate stuck at 60%"
**Your network isn't learning enough. Try:**
```bash
python train.py --mcts-simulations 200 --num-iterations 200
```

## ğŸ“ˆ What to Expect

### After 10 iterations (1-2h)
- Win rate vs random: ~70-80%
- Plays legal moves, basic tactics
- **Not ready for production**

### After 50 iterations (6-8h)
- Win rate vs random: ~90-95%
- Understands captures, chains
- **Good for testing**

### After 100 iterations (12-24h)
- Win rate vs random: ~95-98%
- Strategic play, setup moves
- **Ready for production** âœ…

### After 200+ iterations (24-48h)
- Win rate vs random: ~99%+
- Near-optimal play
- **Potentially superhuman** ğŸ†

## ğŸ® Next Steps After Training

1. **Test vs Minimax:**
   ```python
   # Add this to train.py to evaluate vs minimax AI
   ```

2. **Export to Browser:**
   ```bash
   python export_model.py --checkpoint checkpoints/songo_latest.pt
   ```

3. **Integrate in App:**
   - Add "Neural AI" difficulty level
   - Load ONNX model with `onnxruntime-web`
   - Enjoy watching it dominate!

4. **Keep Training:**
   - Resume with more iterations
   - Collect user games for supervised fine-tuning
   - Build a leaderboard

## ğŸ¤” Common Questions

**Q: Can I use GPU?**
A: Yes! PyTorch auto-detects GPU. Training will be ~3-5x faster.

**Q: How much better than minimax?**
A: After 100+ iterations, should be significantly stronger. Minimax is limited by hand-crafted evaluation; RL learns optimal evaluation.

**Q: Can I train on Google Colab?**
A: Yes! Copy the `training/` folder to Colab, it has free GPU.

**Q: Will it beat you?**
A: Possibly! With enough training (200+ iterations), it might discover strategies even strong humans don't know.

**Q: Can I stop and resume?**
A: Yes! Use `--resume-from checkpoints/songo_latest.pt`

## ğŸ“š Learn More

- `README.md` - Complete documentation
- `training/*.py` - Well-commented source code
- AlphaZero paper: https://arxiv.org/abs/1712.01815

## ğŸ†˜ Need Help?

Check:
1. `README.md` for detailed docs
2. Run tests: `python songo_env.py`, `python songo_net.py`
3. Review console output for errors

## ğŸ‰ You're Ready!

Start training:
```bash
python train.py
```

Then grab coffee â˜• and wait for the magic to happen! ğŸª„

The AI will:
1. Play thousands of games against itself
2. Learn from every mistake
3. Discover winning patterns
4. Emerge as a formidable opponent

**Good luck!** ğŸ€
